{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOde4bHBGJREATx8j1PGMTd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natybkl/Hate-Speech-Detection-in-Amharic-Language/blob/main/Hate_Speech_Detection_in_Amharic_Language%5BmBERT%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hate-Speech-Detection-in-Amharic-Language using fine-tuned mBERT\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5th2xj-d_jKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This repository contains the code and resources for a machine learning project that uses fine-tuned mBERT to detect hate speech in Amharic language. The model was fine-tuned using the Hugging Face Trainer API."
      ],
      "metadata": {
        "id": "Yadf0cXeB_JA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. mBERT (multilingual BERT) is a pre-trained language model developed by Google that can understand multiple languages. You can find more information about mBERT [here](https://github.com/google-research/bert/blob/master/multilingual.md).\n",
        "2. Davlan has provided a finetuned mBERT model specifically for the Amharic language, which is used for this project. The model is available on Hugging Face [here](https://huggingface.co/Davlan/bert-base-multilingual-cased-finetuned-amharic).\n",
        "3. Since the original mBERT model is not well-suited for Amharic, we fine-tuned Davlan's model specifically for the task of hate speech detection in the Amharic language."
      ],
      "metadata": {
        "id": "tvbcH4PECEf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1**: Installing nexessary packages [Done]"
      ],
      "metadata": {
        "id": "l4RwI8KTDHmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need two major packages:\n",
        "1. Transformers package made available by Huggingface\n",
        "2. Dataset package made availale by Huggingface"
      ],
      "metadata": {
        "id": "H2K1Wzw7DSpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the transformers package\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "11EQe1nxEFQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the datasets package\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "eDYOmQbdEKdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2**: Import relevent libraries from the installed packages [Done]"
      ],
      "metadata": {
        "id": "9ikIe3gvEXaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the datasets package\n",
        "from datasets import Dataset\n",
        "import datasets\n",
        "#import load metric for model evaluation\n",
        "from datasets import load_metric"
      ],
      "metadata": {
        "id": "-be0cVn-EqgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy and pandas for mathematical computation and data manipulation respectively \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import drive package to connect this colab file with the drive where the data will be retrived from\n",
        "from google.colab import drive\n",
        "#import the pipeline of transformers\n",
        "from transformers import pipeline\n",
        "#import AutoTokenizer for tokenization purposes\n",
        "from transformers import AutoTokenizer\n",
        "#import the Trainer API\n",
        "from transformers import TrainingArguments, Trainer\n",
        "#import early stopping callback \n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n"
      ],
      "metadata": {
        "id": "Pwch8a08EsVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3**: Import dataset to be used for training,validating and testing the model [Done]\n"
      ],
      "metadata": {
        "id": "v0SuSmmkE17B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used for this project is an Amharic dataset that was made available by Data Mendeley. It contains Amharic posts and comments retrieved from Facebook. It has 30,000 rows. The dataset can be accessed from [here](https://data.mendeley.com/datasets/ymtmxx385m)"
      ],
      "metadata": {
        "id": "ym07-dX9FGi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mount google drive to access the dataset directly from drive\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "id": "2gPPFLXfFLLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fetch the dataset from drive\n",
        "Labels = pd.read_csv('/content/drive/MyDrive/Machine Learning/Data/Amharic-Hate-Speech-Dataset/Labels.txt',header=None)\n",
        "Posts = pd.read_csv('/content/drive/MyDrive/Machine Learning/Data/Amharic-Hate-Speech-Dataset/Posts.txt',header=None)"
      ],
      "metadata": {
        "id": "uifdulC_FVuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4**: Preprocess the Dataset [In-progress]"
      ],
      "metadata": {
        "id": "D6M91tBwGdrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the dataset was retrived, the labels and the post were in different files. \n",
        "\n",
        "\n",
        "1. Hence, the first step in this phase is merging the files into one panda's dataframe.\n",
        "2. Second step is Label encoding. Lable encoding is the process of converting the labels(classes) into numeric format to make it easier for the machine to understand it\n",
        "3. Third step is dividing the dataset into training, validation and testing categories. The division ratio is 7:1:2 respectively.\n",
        "4. Last step is to remove an unncessary columns from the main dataset and merging the all the categories into one main dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "P4oszitRGhFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#naming the columns\n",
        "Labels.columns = [\"labels\"]\n",
        "Texts.columns = [\"Texts\"]"
      ],
      "metadata": {
        "id": "paQwjZQ2GrNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding the classes into numerical data\n",
        "Labels = Labels.replace(['Free', 'Free ','Hate'],[0,0,1]) "
      ],
      "metadata": {
        "id": "MryK2sOfGtKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the encoded label data\n",
        "Labels.head(10)"
      ],
      "metadata": {
        "id": "PLJgtD5NGzyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the Amharic data\n",
        "Texts.head(1000)"
      ],
      "metadata": {
        "id": "AWRUGm96G2eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge the datasets\n",
        "Frames = [Labels, Texts]\n",
        "Merged = pd.concat(Frames, axis=1)"
      ],
      "metadata": {
        "id": "Eg-XBTNKG6Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preview of merged data\n",
        "Merged"
      ],
      "metadata": {
        "id": "uQv4YLHAG9e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide the dataset into train, validation and test categories \n",
        "train_val_df, test_dataset = train_test_split(Merged, test_size=0.20, random_state=42)\n",
        "train_dataset, evaluation_dataset = train_test_split(train_val_df, test_size=0.115, random_state=42)\n",
        "print('Training dataset shape: ', train_dataset.shape)\n",
        "print('Validation dataset shape: ', evaluation_dataset.shape)\n",
        "print('Testing dataset shape: ', test_dataset.shape)"
      ],
      "metadata": {
        "id": "Mv2WMm7eHBRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
        "test_dataset=Dataset.from_pandas(test_dataset)"
      ],
      "metadata": {
        "id": "uuaXpYG8HHtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
        "train_dataset=Dataset.from_pandas(train_dataset)"
      ],
      "metadata": {
        "id": "26VomEhQHLhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
        "evaluation_dataset=Dataset.from_pandas(evaluation_dataset)"
      ],
      "metadata": {
        "id": "L0yommwXHNAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preview of the dataset after conversion\n",
        "(test_dataset)"
      ],
      "metadata": {
        "id": "pVO1JLMKHPw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5**: Tokenizing Dataset [TODO]"
      ],
      "metadata": {
        "id": "2wA2hbTMHUdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6**: Prepare the tokenized Dataset [TODO]"
      ],
      "metadata": {
        "id": "mCubgXe1HgNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 7**: Fine-tune the model [TODO]"
      ],
      "metadata": {
        "id": "zIFqBq9dHlKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 8**: Test the model [TODO]"
      ],
      "metadata": {
        "id": "99z_DK84HpjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 9**: Push the model to Huggingface Hub [TODO]"
      ],
      "metadata": {
        "id": "4d5DBmO5HxOG"
      }
    }
  ]
}