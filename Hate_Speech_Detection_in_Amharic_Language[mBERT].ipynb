{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNF5t1fHQOD43IFJqP3YFJP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natybkl/Hate-Speech-Detection-in-Amharic-Language/blob/main/Hate_Speech_Detection_in_Amharic_Language%5BmBERT%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hate-Speech-Detection-in-Amharic-Language using fine-tuned mBERT\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5th2xj-d_jKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This repository contains the code and resources for a machine learning project that uses fine-tuned mBERT to detect hate speech in Amharic language. The model was fine-tuned using the Hugging Face Trainer API."
      ],
      "metadata": {
        "id": "Yadf0cXeB_JA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. mBERT (multilingual BERT) is a pre-trained language model developed by Google that can understand multiple languages. You can find more information about mBERT [here](https://github.com/google-research/bert/blob/master/multilingual.md).\n",
        "2. Davlan has provided a finetuned mBERT model specifically for the Amharic language, which is used for this project. The model is available on Hugging Face [here](https://huggingface.co/Davlan/bert-base-multilingual-cased-finetuned-amharic).\n",
        "3. Since the original mBERT model is not well-suited for Amharic, we fine-tuned Davlan's model specifically for the task of hate speech detection in the Amharic language."
      ],
      "metadata": {
        "id": "tvbcH4PECEf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1**: Installing nexessary packages"
      ],
      "metadata": {
        "id": "l4RwI8KTDHmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need two major packages:\n",
        "1. Transformers package made available by Huggingface\n",
        "2. Dataset package made availale by Huggingface"
      ],
      "metadata": {
        "id": "H2K1Wzw7DSpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the transformers package\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "11EQe1nxEFQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the datasets package\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "eDYOmQbdEKdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2**: Import relevent libraries from the installed packages"
      ],
      "metadata": {
        "id": "9ikIe3gvEXaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the datasets package\n",
        "from datasets import Dataset\n",
        "import datasets\n",
        "#import load metric for model evaluation\n",
        "from datasets import load_metric"
      ],
      "metadata": {
        "id": "-be0cVn-EqgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy and pandas for mathematical computation and data manipulation respectively \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import drive package to connect this colab file with the drive where the data will be retrived from\n",
        "from google.colab import drive\n",
        "#import the pipeline of transformers\n",
        "from transformers import pipeline\n",
        "#import AutoTokenizer for tokenization purposes\n",
        "from transformers import AutoTokenizer\n",
        "#import the Trainer API\n",
        "from transformers import TrainingArguments, Trainer\n",
        "#import early stopping callback \n",
        "from transformers import EarlyStoppingCallback, IntervalStrategy\n"
      ],
      "metadata": {
        "id": "Pwch8a08EsVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3**: Import dataset to be used for training,validating and testing the model\n"
      ],
      "metadata": {
        "id": "v0SuSmmkE17B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset used for this project is an Amharic dataset that was made available by Data Mendeley. It contains Amharic posts and comments retrieved from Facebook. It has 30,000 rows. The dataset can be accessed from [here](https://data.mendeley.com/datasets/ymtmxx385m)"
      ],
      "metadata": {
        "id": "ym07-dX9FGi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mount google drive to access the dataset directly from drive\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "id": "2gPPFLXfFLLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fetch the dataset from drive\n",
        "Labels = pd.read_csv('/content/drive/MyDrive/Machine Learning/Data/Amharic-Hate-Speech-Dataset/Labels.txt',header=None)\n",
        "Posts = pd.read_csv('/content/drive/MyDrive/Machine Learning/Data/Amharic-Hate-Speech-Dataset/Posts.txt',header=None)"
      ],
      "metadata": {
        "id": "uifdulC_FVuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4**: Preprocess the Dataset"
      ],
      "metadata": {
        "id": "D6M91tBwGdrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the dataset was retrived, the labels and the post were in different files. \n",
        "\n",
        "\n",
        "1. Hence, the first step in this phase is merging the files into one panda's dataframe.\n",
        "2. Second step is Label encoding. Lable encoding is the process of converting the labels(classes) into numeric format to make it easier for the machine to understand it\n",
        "3. Third step is dividing the dataset into training, validation and testing categories. The division ratio is 7:1:2 respectively.\n",
        "4. Last step is to remove an unncessary columns from the main dataset and merging the all the categories into one main dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "P4oszitRGhFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#naming the columns\n",
        "Labels.columns = [\"labels\"]\n",
        "Texts.columns = [\"Texts\"]"
      ],
      "metadata": {
        "id": "paQwjZQ2GrNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding the classes into numerical data\n",
        "Labels = Labels.replace(['Free', 'Free ','Hate'],[0,0,1]) "
      ],
      "metadata": {
        "id": "MryK2sOfGtKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the encoded label data\n",
        "Labels.head(10)"
      ],
      "metadata": {
        "id": "PLJgtD5NGzyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the Amharic data\n",
        "Texts.head(1000)"
      ],
      "metadata": {
        "id": "AWRUGm96G2eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merge the datasets\n",
        "Frames = [Labels, Texts]\n",
        "Merged = pd.concat(Frames, axis=1)"
      ],
      "metadata": {
        "id": "Eg-XBTNKG6Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preview of merged data\n",
        "Merged"
      ],
      "metadata": {
        "id": "uQv4YLHAG9e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide the dataset into train, validation and test categories \n",
        "train_val_df, test_dataset = train_test_split(Merged, test_size=0.20, random_state=42)\n",
        "train_dataset, evaluation_dataset = train_test_split(train_val_df, test_size=0.115, random_state=42)\n",
        "print('Training dataset shape: ', train_dataset.shape)\n",
        "print('Validation dataset shape: ', evaluation_dataset.shape)\n",
        "print('Testing dataset shape: ', test_dataset.shape)"
      ],
      "metadata": {
        "id": "Mv2WMm7eHBRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
        "test_dataset=Dataset.from_pandas(test_dataset)"
      ],
      "metadata": {
        "id": "uuaXpYG8HHtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
        "train_dataset=Dataset.from_pandas(train_dataset)"
      ],
      "metadata": {
        "id": "26VomEhQHLhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the format of the dataset to HuggingFace Dataset from Pandas DataFrame\n",
        "evaluation_dataset=Dataset.from_pandas(evaluation_dataset)"
      ],
      "metadata": {
        "id": "L0yommwXHNAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preview of the dataset after conversion\n",
        "(test_dataset)"
      ],
      "metadata": {
        "id": "pVO1JLMKHPw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine the train and test dataset into one datset\n",
        "main_dataset= datasets.DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'test': test_dataset,\n",
        "    'evaluate': evaluation_dataset\n",
        "})"
      ],
      "metadata": {
        "id": "zZpefHw6F7vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preview of the dataset after merging\n",
        "main_dataset"
      ],
      "metadata": {
        "id": "gl4hUZl5GA5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and testing data size\n",
        "training_data_size = main_dataset['train'].num_rows\n",
        "testing_data_size = main_dataset['test'].num_rows\n",
        "evaluation_data_size = main_dataset['evaluate'].num_rows"
      ],
      "metadata": {
        "id": "D0zS8ezvGGwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5**: Tokenizing Dataset"
      ],
      "metadata": {
        "id": "2wA2hbTMHUdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Tokenizer is used to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data.\n",
        "\n",
        "In this case, the tokenizer used is an AutoTokenizer from the fine-tuned mBERT model made available by Hugging face [here](https://huggingface.co/Davlan/bert-base-multilingual-cased-finetuned-amharic)\n",
        "\n",
        "In this phase, we have the following tasks:\n",
        "\n",
        "1. Load the tokenizer\n",
        "2. Create a tokenizer function that takes the dataset in batches and tokenize them using the tokenizer loaded from the model\n",
        "3. Call the tokenizer function on the whole dataset"
      ],
      "metadata": {
        "id": "gS6Q3-RnGMiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading a tokenizer from the pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/bert-base-multilingual-cased-finetuned-amharic\")"
      ],
      "metadata": {
        "id": "Vf_W481xHANj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Have a tokenizer function that uses the tokenizer \n",
        "def tokenize_function(data):\n",
        "    return tokenizer(data[\"post\"], padding=\"max_length\", truncation=True)"
      ],
      "metadata": {
        "id": "EcDN0FX7HTvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize all the data using the mapping functionality\n",
        "tokenized_datasets = main_dataset.map(tokenize_function)"
      ],
      "metadata": {
        "id": "m2DqaDa3HUdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#empty cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "nlup3-71HXV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6**: Prepare the tokenized Dataset"
      ],
      "metadata": {
        "id": "mCubgXe1HgNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this phase, we do the following tasks:\n",
        "\n",
        "1. Remove unnecessary columns such as the \"posts\" column from the tokenized dataset as we no longer need them\n",
        "2. Change the format of the tokenized dataset into pytorch since we are using pytorch\n",
        "3. Load the dataset using DataLoader with the proper batch size\n",
        "4. Preview the features of the dataset to make sure everything is okay"
      ],
      "metadata": {
        "id": "tf1TajuvHdgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove the posts column as it is no longer needed\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"post\"])"
      ],
      "metadata": {
        "id": "c0PFvVmAHwCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing the format of the tokenized dataset to torch\n",
        "tokenized_datasets.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "0TcGFsY7HxsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffeling and selecting the needed size of dataset for training and evaluating the model\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(training_data_size))\n",
        "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(testing_data_size))\n",
        "small_eval_dataset = tokenized_datasets[\"evaluate\"].shuffle(seed=42).select(range(evaluation_data_size))"
      ],
      "metadata": {
        "id": "ghxQQwqhH0Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preview of the shuffeled and selected evaluation dataset\n",
        "small_eval_dataset\n",
        "     "
      ],
      "metadata": {
        "id": "ueUeN1BYH1-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preview of the shuffeled and selected training dataset\n",
        "small_train_dataset"
      ],
      "metadata": {
        "id": "YbPde7y-H34D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preview of the shuffeled and selected testing dataset\n",
        "small_test_dataset"
      ],
      "metadata": {
        "id": "NXVeOw4mH6LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the dataset using DataLoader\n",
        "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=4)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=4)\n",
        "test_dataloader = DataLoader(small_test_dataset, batch_size=4)"
      ],
      "metadata": {
        "id": "I2x6bhzZH8f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 7**: Fine-tune the model"
      ],
      "metadata": {
        "id": "zIFqBq9dHlKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This phase has the following steps:\n",
        "\n",
        "1. Load the model\n",
        "2. Specify the computing metric\n",
        "3. Specify the Training/fine-tuning arguments\n",
        "4. Load the Trainer class\n",
        "5. Fine-tune the model\n",
        "\n",
        "7.1 **Load the model**\n",
        "\n",
        "We load the fine-tuned mBERT mode in this step.\n"
      ],
      "metadata": {
        "id": "6uqn7vyEIAhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load auto mode classifier from the pretrained model \n",
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Davlan/bert-base-multilingual-cased-finetuned-amharic\", num_labels=2)\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"Davlan/\", num_labels=2)\n",
        " "
      ],
      "metadata": {
        "id": "fNYm0XM9Iohp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.2 **Computing Metrics**\n",
        "\n",
        "In this stage, we load the computing metrics. The computing metrics used in this phase are the f1-score and the accuracy. These computing metrics are used during the validation and testing phase."
      ],
      "metadata": {
        "id": "_v7bE4UQIrOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"f1\",\"accuracy\")"
      ],
      "metadata": {
        "id": "8oMw89JPIzYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that uses the loaded metrics to compute the performance of the model\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "MQxoIkxmJEMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.3 **Specify the training arguments**\n",
        "\n",
        "This phase includes loading the training parameters and hyperparameters. It also specifies the validation interval during the fine-tuning process."
      ],
      "metadata": {
        "id": "icKFeVQEJI7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\")"
      ],
      "metadata": {
        "id": "YiC-30aCJbBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EarlyStoppingCallback, IntervalStrategy"
      ],
      "metadata": {
        "id": "UhmUsAcfJbTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "   f\"training_with_callbacks\",\n",
        "   evaluation_strategy = IntervalStrategy.STEPS, # \"steps\"\n",
        "   warmup_steps=500,                # number of warmup steps for learning rate  \n",
        "   save_steps=2000,\n",
        "   eval_steps = 2000, # Evaluation and Save happens every 2000 steps\n",
        "   save_total_limit = 3, # Only last 3 models are saved. Older ones are deleted.\n",
        "   learning_rate=1e-5,\n",
        "   per_device_train_batch_size=4,\n",
        "   per_device_eval_batch_size=4,\n",
        "   num_train_epochs=15,\n",
        "   weight_decay=0.01,\n",
        "   push_to_hub=False,\n",
        "   metric_for_best_model = 'f1',\n",
        "   do_predict=True,\n",
        "   load_best_model_at_end=True)"
      ],
      "metadata": {
        "id": "02O2w_ZhJpPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.4 **Load the Trainer class**\n",
        "\n",
        "In the trainer class, early stopping strategy is called. Early Stopping is a an optimization technique used to reduce overfitting without compromising on model accuracy. It allows to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset. For this model, the early stopping patience used is 10 epoches."
      ],
      "metadata": {
        "id": "w-7C8i3WJr2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks = [EarlyStoppingCallback(early_stopping_patience=10)],\n",
        ")"
      ],
      "metadata": {
        "id": "cA3jznqKJ17Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.5 **Fine-tune the model**\n",
        "\n",
        "Fine-tuning process embbeds the validation within itself. After every 2000 steps of finetuning, the model is validated on the loaded computing metrics to modify the hyperparameters to make the model perform well."
      ],
      "metadata": {
        "id": "hAI4z3bQJ6ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "cmExKvSgKBS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 8**: Test the model"
      ],
      "metadata": {
        "id": "99z_DK84HpjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this stage the model is tested on the testing dataset. This dataset isn't seen by the model during the finetuning process."
      ],
      "metadata": {
        "id": "VRU-h426KId9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(small_test_dataset)"
      ],
      "metadata": {
        "id": "SKpClFqxKLEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 9**: Push the model to Huggingface Hub"
      ],
      "metadata": {
        "id": "4d5DBmO5HxOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final model was pushed and made publicly available on Huggingface. You can find the model on huggingface [here](https://huggingface.co/NathyB/Hate-Speech-Detection-in-Amharic-Language-mBERT)."
      ],
      "metadata": {
        "id": "5Xfe83PoKRP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install huggingface_hub package to interact with huggingface platform\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "nfYxErNgKYVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "Py90eIFdKa4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "0MsqkQqhKcpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#specify the path for the trainde model and tokenizer to huggingface repository\n",
        "model.save_pretrained(\"path/to/improved-amharic-hate-speech-detection-mBERT\")\n",
        "tokenizer.save_pretrained(\"path/to/improved-amharic-hate-speech-detection-mBERT\")"
      ],
      "metadata": {
        "id": "D6jVBTA9K58W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#push the trained model to huggingface repository\n",
        "model.push_to_hub(\"improved-amharic-hate-speech-detection-mBERT\")"
      ],
      "metadata": {
        "id": "lmHOplARK84b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#push the tokenizer to huggingface repository\n",
        "tokenizer.push_to_hub(\"improved-amharic-hate-speech-detection-mBERT\")"
      ],
      "metadata": {
        "id": "Yjd_MzplK-ev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}